#!/bin/bash

#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --time=0-12:00:00
#SBATCH --mem=4G
#SBATCH --output=Lucky_exvivo1_slice_%a.out
#SBATCH --mail-user=thomas.j.aiman@vanderbilt.edu
#SBATCH --mail-type=ALL
#SBATCH --array=1-257%200

# the '#!/bin/bash' tells the system what shell to use when interpreting our code (we're using bash)
# SLURM reads the sbatch commands but unix ignores them becuase they are behind # symbols

#'array' means that this code is run by SLURM as a parallel for loop. Each iteration of the loop assigns 
#a new task to a new processor and assigns one of the array numbers to the variable SLURM_ARRAY_TASK_ID. 
#We can later pass SLURM_ARRAY_TASK_ID to MATLAB to control the slice number.
#
#-edit array to be whatever slice numbers you want to run. The %200 dictates how many of these tasks 
#can be handled simultaneously. If you have a limited amount of processors available for your 
#lab/company/group, you might not want to hog all the resources so other people can still do stuff.
#
#-edit 'output' to be what you want your .out files to be. Or get rid of it entirely so you don't get 
#.out files. The %a references the array number that is running.
#
#-edit 'mail-user' to your email address. It will tell you when the job starts, finishes, and if it fails.
#
#-leave ntasks=1 because a server using the X window system for graphics is going to be trash at 
#rendering graphics and there's nothing we can do about it

#------------------------------- User Inputs -----------------------------------------------------
#'export' creates an environment variable. This is necessary to pass information to MATLAB later.
#to get around ACCRE file system limitations and improve speed, we are going to do all our work in a 
#temporary folder on the node that the job gets assigned to. Name this whatever you want, but it should be 
#unique because other people will also be making folders in /tmp/
export WORK_DIR=/tmp/aimantj

#these variables pass information to MATLAB later. 
#set DATASETNAME to be whatever you want appended to the beginning of your files/folders
export DATASETNAME=Lucky_exvivo1_DTI
#set MATFILEPATH to be the location of the .mat file that has tensors_block
export MATFILEPATH=/scratch/aimantj/DTI_blocks/Lucky_exvivo1_block.mat
#set WRAPPERFOLDER to be the folder that has 'DTI_glyphs_wrapper_slurm.m' and 'codefiles' in it
export WRAPPERFOLDER=/scratch/aimantj/DTI_glyph_maker
#set STOR_DIR to be where you want your finished .tar.gz files to be put after we are done. We have to 
#move the files because the /tmp/ folder is erased periodically and it only exists on that compute node; 
#it isn't shared. There's no way for us to get back to it.
export STOR_DIR=/scratch/aimantj
#---------------------------------------------------------------------------------------------------

#make our temporary folder. The $ means that the system uses the value of the variable WORK_DIR. Without 
#it, our folder would literally end up being called 'WORK_DIR'.
mkdir -p $WORK_DIR

#'echo' prints text to the terminal for you to see on screen. In our case it will print to the .out file 
#generated by slurm. Not necessary but useful if you need to debug. 
echo "SLURM_JOBID: " $SLURM_JOBID
echo "SLURM_ARRAY_TASK_ID: " $SLURM_ARRAY_TASK_ID
echo "SLURM_ARRAY_JOB_ID: " $SLURM_ARRAY_JOB_ID

echo "WORK_DIR: " $WORK_DIR
echo "DATASETNAME: " $DATASETNAME
echo "MATFILEPATH: " $MATFILEPATH
echo "WRAPPERFOLDER: " $WRAPPERFOLDER
echo "STOR_DIR: " $STOR_DIR

#this line uses LMOD which runs on ACCRE to handle different pieces of software. It essentially adds 
#MATLAB to our shell search path.
module load MATLAB

#starts up MATLAB. The -r is an option that allows you to specify a line to run in the MATLAB shell.
matlab -r "DTI_glyphs_wrapper_slurm"





